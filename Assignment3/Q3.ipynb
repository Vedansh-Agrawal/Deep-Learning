{"metadata":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6959697,"sourceType":"datasetVersion","datasetId":3997887}],"dockerImageVersionId":30579,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Deep Learning\n\n## Assignment 3\n\n### Question 3\n### Encoder Decoder with Attention","metadata":{}},{"cell_type":"markdown","source":"In this question of the assignment, we will be implementing a Encoder Decoder model with Attention. Let us begin by loading the data give to us","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport random\nimport torch.functional as F\nimport re\nfrom collections import Counter\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, TensorDataset","metadata":{"execution":{"iopub.status.busy":"2023-11-13T17:20:07.957370Z","iopub.execute_input":"2023-11-13T17:20:07.957865Z","iopub.status.idle":"2023-11-13T17:20:07.964826Z","shell.execute_reply.started":"2023-11-13T17:20:07.957827Z","shell.execute_reply":"2023-11-13T17:20:07.963280Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2023-11-13T17:20:08.329075Z","iopub.execute_input":"2023-11-13T17:20:08.329479Z","iopub.status.idle":"2023-11-13T17:20:08.335255Z","shell.execute_reply.started":"2023-11-13T17:20:08.329448Z","shell.execute_reply":"2023-11-13T17:20:08.334106Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def separate_words_and_special_chars(input_string):\n    # Define a regular expression pattern to match words and special characters\n    pattern = r'([^\\w\\s])|(\\s+)'\n    \n    # Use re.split() to separate words and special characters\n    result = re.split(pattern, input_string)\n    \n    # Remove empty strings from the result list\n    result = [item for item in result if item]\n\n    return result\n\ndef separate_words_and_special_chars_1(input_string):\n    # Define a regular expression pattern to match words and special characters\n    pattern = r'([^\\w\\s])|(\\s+)'\n    \n    # Use re.split() to separate words and special characters\n    result = re.split(pattern, input_string)\n    \n    # Remove empty strings from the result list\n    result = [item for item in result if item]\n    result = ['<SRT>'] + result + ['<END>']\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-11-13T17:20:08.530876Z","iopub.execute_input":"2023-11-13T17:20:08.531324Z","iopub.status.idle":"2023-11-13T17:20:08.541403Z","shell.execute_reply.started":"2023-11-13T17:20:08.531291Z","shell.execute_reply":"2023-11-13T17:20:08.539629Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"with open(\"/kaggle/input/deep-leanring3/train.sources\", \"r\") as file:\n    X_train = file.readlines()\n    X_train = [separate_words_and_special_chars(string) for string in X_train]\n\nwith open(\"/kaggle/input/deep-leanring3/train.targets\", \"r\") as file:\n    Y_train = file.readlines()\n    Y_train = [separate_words_and_special_chars_1(string) for string in Y_train]\n\n\nwith open(\"/kaggle/input/deep-leanring3/dev.sources\", \"r\") as file:\n    X_val = file.readlines()\n    X_val = [separate_words_and_special_chars(string) for string in X_val]\n\nwith open(\"/kaggle/input/deep-leanring3/dev.targets\", \"r\") as file:\n    Y_val = file.readlines()\n    Y_val = [separate_words_and_special_chars_1(string) for string in Y_val]\n\nwith open(\"/kaggle/input/deep-leanring3/test.sources\", \"r\") as file:\n    X_test = file.readlines()\n    X_test = [separate_words_and_special_chars(string) for string in X_test]\n\nwith open(\"/kaggle/input/deep-leanring3/test.targets\", \"r\") as file:\n    Y_test = file.readlines()\n    Y_test = [separate_words_and_special_chars_1(string) for string in Y_test]\n","metadata":{"execution":{"iopub.status.busy":"2023-11-13T17:20:08.707731Z","iopub.execute_input":"2023-11-13T17:20:08.708184Z","iopub.status.idle":"2023-11-13T17:20:53.147599Z","shell.execute_reply.started":"2023-11-13T17:20:08.708148Z","shell.execute_reply":"2023-11-13T17:20:53.146380Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"input_vocab = {token: idx + 3 for idx, (token, _) in enumerate(Counter([item for sublist in X_train for item in sublist]).most_common())}\noutput_vocab = {token: idx + 1 for idx, (token, _) in enumerate(Counter([item for sublist in Y_train for item in sublist]).most_common())}\n\noutput_vocab['<PAD>'] = 0\ninput_vocab['<PAD>'] = 0\ninput_vocab['<SRT>'] = 1\ninput_vocab['<END>'] = 2","metadata":{"execution":{"iopub.status.busy":"2023-11-13T17:32:52.261682Z","iopub.execute_input":"2023-11-13T17:32:52.262113Z","iopub.status.idle":"2023-11-13T17:32:57.964617Z","shell.execute_reply.started":"2023-11-13T17:32:52.262080Z","shell.execute_reply":"2023-11-13T17:32:57.963201Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"X_train_numerical = [[input_vocab[token] for token in X] for X in X_train]\nY_train_numerical = [[output_vocab[token] for token in X] for X in Y_train]","metadata":{"execution":{"iopub.status.busy":"2023-11-13T17:32:57.967094Z","iopub.execute_input":"2023-11-13T17:32:57.967501Z","iopub.status.idle":"2023-11-13T17:33:04.379385Z","shell.execute_reply.started":"2023-11-13T17:32:57.967467Z","shell.execute_reply":"2023-11-13T17:33:04.378171Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"max_length = 500\nX_padded = pad_sequence([torch.tensor(X_train_numerical[i][:max_length]) for i in range(len(X_train_numerical))], padding_value=0).T\nY_padded = pad_sequence([torch.tensor(Y_train_numerical[i][:max_length]) for i in range(len(Y_train_numerical))], padding_value=0).T","metadata":{"execution":{"iopub.status.busy":"2023-11-13T17:33:04.380887Z","iopub.execute_input":"2023-11-13T17:33:04.381248Z","iopub.status.idle":"2023-11-13T17:33:28.333241Z","shell.execute_reply.started":"2023-11-13T17:33:04.381219Z","shell.execute_reply":"2023-11-13T17:33:28.331845Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"train_dataset = TensorDataset(X_padded[:128].to(device), Y_padded[:128].to(device))\n\nbatch_size = 32\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T17:38:27.838807Z","iopub.execute_input":"2023-11-13T17:38:27.839253Z","iopub.status.idle":"2023-11-13T17:38:27.846625Z","shell.execute_reply.started":"2023-11-13T17:38:27.839220Z","shell.execute_reply":"2023-11-13T17:38:27.845198Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"Now Let us start implementing the model class","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_vocab_size, emb_dim, enc_hid_dim, num_layers, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(input_vocab_size, emb_dim)\n        self.encoder = nn.LSTM(emb_dim, enc_hid_dim, num_layers=num_layers, bidirectional=True)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, src): # Shape of src is (sequence_length, batch_size)\n        embedded = self.dropout(self.embedding(src))\n        outputs, (hidden, cell) = self.encoder(embedded)\n        return outputs, hidden, cell #Hidden shape is (sequence_length, batch_size, 2*hidden_size)\n\nclass Attention(nn.Module):\n    def __init__(self, enc_hid_dim, dec_hid_dim):\n        super().__init__()\n        self.attn = nn.Linear((enc_hid_dim * 2) + (2*dec_hid_dim), dec_hid_dim)\n        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n\n    def forward(self, hidden, encoder_outputs): #encoder outputs (sequence_length, batch_size, 2*hidden_size)\n        src_len = encoder_outputs.shape[0]\n        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n        energy = torch.tanh(self.attn(torch.cat((hidden.permute(1, 0, 2), encoder_outputs), dim=2)))\n        attention = self.v(energy).squeeze(2)\n        return torch.softmax(attention, dim=1) #(src_len, batch_size)\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, num_layers, dropout, attention):\n        super().__init__()\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.rnn = nn.LSTM((enc_hid_dim * 2) + emb_dim, dec_hid_dim, num_layers=num_layers)\n        self.output_dim = output_dim\n        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n        self.attention = attention\n\n    def forward(self, input, hidden, cell, encoder_outputs):\n        input = input.unsqueeze(0) #(1, batch_size)\n        embedded = self.dropout(self.embedding(input)) #(1, batch_size, emb_dim)\n        a = self.attention(hidden.view(batch_size, -1), encoder_outputs)\n        a = a.unsqueeze(1).permute(2, 1, 0) #(batch_size, 1, src_len)\n        encoder_outputs = encoder_outputs.permute(1, 0, 2) #(batch_size, seq_len, 2*enc_hid_dim)\n        weighted = torch.bmm(a, encoder_outputs)#(batch_size, 1, 2*enc_hid_dim)\n        weighted = weighted.permute(1, 0, 2)#(1, batch_size, 2*enc_hid_dim)\n        rnn_input = torch.cat((embedded, weighted), dim=2) #(1, batch_size, 2*hid_dim + emb_dim)\n        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n        embedded = embedded.squeeze(0) #(batch_size, emb_dim)\n        output = output.squeeze(0) #(batch_size, dec_hid_dim)\n        weighted = weighted.squeeze(0) #(batch_size, 2*enc_hid_dim)\n        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1)) #(batch_size, output_dim)\n        return prediction, hidden, cell\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device, beam_width):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n        self.beam_width = beam_width\n\n    def forward(self, src, trg, teacher_forcing=True):\n        trg_len = trg.shape[0] - 1 # trg shape = (seq_len, batch_size)\n        batch_size = trg.shape[1]\n        trg_vocab_size = self.decoder.output_dim\n        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n        encoder_outputs, hidden, cell = self.encoder(src)\n        hidden = hidden[:2]\n        cell = cell[:2]\n        if teacher_forcing:\n            for t in range(0, trg_len):\n                input = trg[t]\n                output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n                outputs[t] = output\n\n        else:\n            beam_search = BeamSearch(self.decoder, hidden, cell, self.beam_width, trg_vocab_size)\n            for t in range(0, trg_len):\n                beam_search.step(hidden, cell, encoder_outputs)\n            outputs = beam_search.get_best_sequence()\n        return outputs\n\nclass BeamSearch(nn.Module):\n    def __init__(self, decoder, initial_hidden, initial_cell, beam_width, vocab_size):\n        self.decoder = decoder\n        self.beam_width = beam_width\n        self.vocab_size = vocab_size\n        self.topk = 1 \n\n        self.finished_sequences = []\n        self.topk_sequences = [{\"sequence\": torch.zeros(initial_hidden.shape[1], 1), \"score\": 0.0,\n                                \"hidden\": initial_hidden, \"cell\": initial_cell} for _ in range(beam_width)]\n\n    def step(self, hidden, cell, encoder_outputs):\n        candidates = []\n        for sequence in self.topk_sequences:\n            prev_output = sequence[\"sequence\"][:, -1]\n            output, hidden, cell = self.decoder(prev_output, sequence[\"hidden\"], sequence[\"cell\"], encoder_outputs)\n            log_probs = F.log_softmax(output, dim=1)\n            topk_probs, topk_indices = torch.topk(log_probs, self.beam_width, dim=1)\n\n            for i in range(self.beam_width):\n                candidate = {\n                    \"sequence\": torch.cat((sequence[\"sequence\"], topk_indices[:, i]), dim=1),\n                    \"score\": sequence[\"score\"] + topk_probs[:, i].item(),\n                    \"hidden\": hidden,\n                    \"cell\": cell\n                }\n                candidates.append(candidate)\n\n        candidates.sort(key=lambda x: x[\"score\"], reverse=True)\n        self.topk_sequences = candidates[:self.beam_width]\n\n        return self.topk_sequences[0][\"sequence\"][:, -1].unsqueeze(0), self.topk_sequences[0][\"hidden\"], self.topk_sequences[0][\"cell\"]\n\n    def get_best_sequence(self):\n        return self.topk_sequences[0][\"sequence\"]","metadata":{"execution":{"iopub.status.busy":"2023-11-13T17:38:29.564223Z","iopub.execute_input":"2023-11-13T17:38:29.564699Z","iopub.status.idle":"2023-11-13T17:38:29.602386Z","shell.execute_reply.started":"2023-11-13T17:38:29.564660Z","shell.execute_reply":"2023-11-13T17:38:29.601147Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"Now, let us define the hyperparameters.","metadata":{}},{"cell_type":"code","source":"INPUT_DIM = len(input_vocab)\nOUTPUT_DIM = len(output_vocab)\nEMB_DIM = 512\nENC_HID_DIM = 512\nDEC_HID_DIM = 512\nNUM_LAYERS = 2\nDROPOUT = 0.5\nBEAM_WIDTH = 15\nBATCH_SIZE = 32\nNUM_LAYERS = 2\nNUM_EPOCHS = 4\n\nenc = Encoder(INPUT_DIM, EMB_DIM, ENC_HID_DIM, NUM_LAYERS, DROPOUT)\nattn = Attention(ENC_HID_DIM, DEC_HID_DIM)\ndec = Decoder(OUTPUT_DIM, EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, NUM_LAYERS, DROPOUT, attn)\n\nmodel = Seq2Seq(enc, dec, device, BEAM_WIDTH).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\ncriterion = nn.CrossEntropyLoss(ignore_index=0)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T17:38:29.607351Z","iopub.execute_input":"2023-11-13T17:38:29.607722Z","iopub.status.idle":"2023-11-13T17:38:29.863199Z","shell.execute_reply.started":"2023-11-13T17:38:29.607693Z","shell.execute_reply":"2023-11-13T17:38:29.861932Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"Now let us implement the training process and see how it does.","metadata":{}},{"cell_type":"code","source":"for epoch in range(NUM_EPOCHS):\n    for i, batch in enumerate(train_dataloader):\n        src, trg = batch\n        src = src.T\n        print(src.shape)\n        trg = trg.T\n        output = model(src, trg)\n        output_dim = output.shape[-1]\n        output = output.view(-1, output_dim)\n        \n        trg = trg[1:].reshape(-1)\n        optimizer.zero_grad()\n        loss = criterion(output, trg)\n        loss.backward()\n        optimizer.step()\n\n        if (i+1)%2 == 0:\n            print(f'epoch {epoch+1}/{NUM_EPOCHS}, step {i+1}/{len(train_dataloader)}, loss  {loss.item():.4f}')","metadata":{"execution":{"iopub.status.busy":"2023-11-13T17:38:30.314509Z","iopub.execute_input":"2023-11-13T17:38:30.314937Z","iopub.status.idle":"2023-11-13T17:58:03.426297Z","shell.execute_reply.started":"2023-11-13T17:38:30.314906Z","shell.execute_reply":"2023-11-13T17:58:03.424473Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"torch.Size([242, 32])\ntorch.Size([242, 32])\nepoch 1/4, step 2/4, loss  3.9854\ntorch.Size([242, 32])\ntorch.Size([242, 32])\nepoch 1/4, step 4/4, loss  3.3517\ntorch.Size([242, 32])\ntorch.Size([242, 32])\nepoch 2/4, step 2/4, loss  2.9959\ntorch.Size([242, 32])\ntorch.Size([242, 32])\nepoch 2/4, step 4/4, loss  2.7250\ntorch.Size([242, 32])\ntorch.Size([242, 32])\nepoch 3/4, step 2/4, loss  2.5456\ntorch.Size([242, 32])\ntorch.Size([242, 32])\nepoch 3/4, step 4/4, loss  2.4135\ntorch.Size([242, 32])\ntorch.Size([242, 32])\nepoch 4/4, step 2/4, loss  2.3549\ntorch.Size([242, 32])\ntorch.Size([242, 32])\nepoch 4/4, step 4/4, loss  2.2913\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}